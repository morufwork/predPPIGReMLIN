{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a43338",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6883df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging\n",
    "#import matplotlib.pyplot as plt\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.SASA import ShrakeRupley\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "\n",
    "# Define interaction criteria\n",
    "interaction_criteria = {\n",
    "    \"ARM_STACK\": {\"atomic_type1\": \"ARM\", \"atomic_type2\": \"ARM\", \"min_dist\": 1.5, \"max_dist\": 3.5},\n",
    "    \"H_BOND\": {\"atomic_type1\": \"ACP\", \"atomic_type2\": \"DON\", \"min_dist\": 2.0, \"max_dist\": 3.0},\n",
    "    \"HYDROPHOBIC\": {\"atomic_type1\": \"HPB\", \"atomic_type2\": \"HPB\", \"min_dist\": 2.0, \"max_dist\": 3.8},\n",
    "    \"REPULSIVE\": {\"atomic_type1\": \"POS\", \"atomic_type2\": \"POS\", \"min_dist\": 2.0, \"max_dist\": 6.0},\n",
    "    \"REPULSIVE\": {\"atomic_type1\": \"NEG\", \"atomic_type2\": \"NEG\", \"min_dist\": 2.0, \"max_dist\": 6.0},\n",
    "    \"SALT_BRIDGE\": {\"atomic_type1\": \"POS\", \"atomic_type2\": \"NEG\", \"min_dist\": 2.0, \"max_dist\": 6.0},\n",
    "    \"SS_BRIDGE\": {\"atomic_type1\": \"SG\", \"atomic_type2\": \"SG\", \"min_dist\": 2.0, \"max_dist\": 2.2},\n",
    "}\n",
    "\n",
    "# Full names mapping\n",
    "type_full_names = {\n",
    "    \"ACP\": \"Acceptor\",\n",
    "    \"DON\": \"Donor\",\n",
    "    \"POS\": \"Positive\",\n",
    "    \"NEG\": \"Negative\",\n",
    "    \"HPB\": \"Hydrophobic\",\n",
    "    \"ARM\": \"Aromatic\",\n",
    "    \"HYDROPHOBIC\": \"Hydrophobic\",\n",
    "    \"SALT_BRIDGE\": \"Salt bridge\",\n",
    "    \"ARM_STACK\": \"Aromatic\",\n",
    "    \"H_BOND\": \"Hydrogen bond\",\n",
    "    \"REPULSIVE\": \"Repulsive\",\n",
    "    \"SS_BRIDGE\": \"Disulfide Bridge\"\n",
    "}\n",
    "\n",
    "# Amino acid to atom types mapping (same as given in the provided data)\n",
    "# Amino acid to atom types mapping\n",
    "amino_acid_atoms = {\n",
    "    \"ALA\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\"},\n",
    "    \"ARG\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \"CD\": None, \n",
    "            \"NE\": \"POS,DON\", \"CZ\": \"POS\", \"NH1\": \"POS,DON\", \"NH2\": \"POS,DON\"},\n",
    "    \"ASN\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": None, \n",
    "            \"OD1\": \"ACP\", \"ND2\": \"DON\"},\n",
    "    \"ASP\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": None, \n",
    "            \"OD1\": \"NEG,ACP\", \"OD2\": \"NEG,ACP\"},\n",
    "    \"CYS\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"SG\": \"DON,ACP\"},\n",
    "    \"GLN\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \n",
    "            \"CD\": None, \"OE1\": \"ACP\", \"NE2\": \"DON\"},\n",
    "    \"GLU\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \n",
    "            \"CD\": None, \"OE1\": \"NEG,ACP\", \"OE2\": \"NEG,ACP\"},\n",
    "    \"GLY\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\"},\n",
    "    \"HIS\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"ARM\", \n",
    "            \"ND1\": \"ARM,POS\", \"CD2\": \"ARM\", \"CE1\": \"ARM\", \"NE2\": \"ARM,POS\"},\n",
    "    \"ILE\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG1\": \"HPB\", \n",
    "            \"CG2\": \"HPB\", \"CD1\": \"HPB\"},\n",
    "    \"LEU\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \n",
    "            \"CD1\": \"HPB\", \"CD2\": \"HPB\"},\n",
    "    \"LYS\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \n",
    "            \"CD\": \"HPB\", \"CE\": None, \"NZ\": \"POS,DON\"},\n",
    "    \"MET\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \n",
    "            \"SD\": \"ACP\", \"CE\": \"HPB\"},\n",
    "    \"PHE\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB,ARM\", \n",
    "            \"CD1\": \"HPB,ARM\", \"CD2\": \"HPB,ARM\", \"CE1\": \"HPB,ARM\", \"CE2\": \"HPB,ARM\", \n",
    "            \"CZ\": \"HPB,ARM\"},\n",
    "    \"PRO\": {\"N\": None, \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB\", \n",
    "            \"CD\": None},\n",
    "    \"SER\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": None, \"OG\": \"DON,ACP\"},\n",
    "    \"THR\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": None, \"OG1\": \"DON,ACP\", \n",
    "            \"CG2\": \"HPB\"},\n",
    "    \"TRP\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB,ARM\", \n",
    "            \"CD1\": \"ARM\", \"CD2\": \"HPB,ARM\", \"NE1\": \"ARM,DON\", \"CE2\": \"ARM\", \n",
    "            \"CE3\": \"HPB,ARM\", \"CZ2\": \"HPB,ARM\", \"CZ3\": \"HPB,ARM\", \"CH2\": \"HPB,ARM\"},\n",
    "    \"TYR\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \"CG\": \"HPB,ARM\", \n",
    "            \"CD1\": \"HPB,ARM\", \"CD2\": \"HPB,ARM\", \"CE1\": \"HPB,ARM\", \"CE2\": \"HPB,ARM\", \n",
    "            \"CZ\": \"ARM\", \"OH\": \"DON,ACP\"},\n",
    "    \"VAL\": {\"N\": \"DON\", \"CA\": None, \"C\": None, \"O\": \"ACP\", \"CB\": \"HPB\", \n",
    "            \"CG1\": \"HPB\", \"CG2\": \"HPB\"},\n",
    "}\n",
    "\n",
    "\n",
    "# 1. **Surface Area Residue Calculation using SASA**\n",
    "# Define helper functions for SASA calculation\n",
    "def calculate_sasa(pdb_file):\n",
    "    \"\"\"\n",
    "    Calculate the Solvent Accessible Surface Area (SASA) for residues in a PDB file.\n",
    "    Uses the Shrake-Rupley method.\n",
    "    \"\"\"\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure(pdb_file, pdb_file)\n",
    "    \n",
    "    # Step 2: Initialize the ShrakeRupley class and compute SASA\n",
    "    sr = ShrakeRupley()\n",
    "    sr.compute(structure, level=\"A\")  # Compute at the atom level\n",
    "    \n",
    "    surface_residues = []\n",
    "    residues_dict = {}\n",
    "\n",
    "    for chain in structure[0]:  # Iterate over chains in the first model\n",
    "        for residue in chain:\n",
    "            # Skip any residues that are not standard (e.g., water, ligands)\n",
    "            if residue.get_resname() in [\"WAT\", \"HOH\", \"ACE\"]:\n",
    "                continue\n",
    "\n",
    "            # Accumulate SASA for all atoms in the residue\n",
    "            residue_sasa = sum(atom.sasa for atom in residue if hasattr(atom, 'sasa'))\n",
    "            id = f\"{residue.get_resname()} {residue.id[1]}\"\n",
    "            residues_dict[residue] = residue_sasa\n",
    "            \n",
    "    # # Step 3:Identify the maximum SASA: Find the maximum SASA value across all residues in the protein structure\n",
    "    max_sasa = max(residues_dict.values())\n",
    "        \n",
    "    # Step 4: Set a threshold for identifying surface residues\n",
    "    surface_threshold = 0.25 * max_sasa\n",
    "    \n",
    "    # Step 5: Identify and collect surface residues\n",
    "    for sasa_residue in residues_dict.items():\n",
    "        if sasa_residue[1] >= surface_threshold:\n",
    "            surface_residues.append(sasa_residue[0])\n",
    "            #print(sasa_residue)\n",
    "\n",
    "    return surface_residues\n",
    "\n",
    "\n",
    "# Step 1: Parse PDB File\n",
    "def parse_pdb(pdb_file):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_file, pdb_file)\n",
    "    residues = {}\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                res_name = residue.get_resname()\n",
    "                res_id = residue.get_id()[1]  # Residue sequence number\n",
    "                res_key = f\"{res_name}_{res_id}\"\n",
    "                if res_key not in residues:\n",
    "                    residues[res_key] = {\n",
    "                        'residue': res_name,\n",
    "                        'res_id': res_id,\n",
    "                        'atoms': []\n",
    "                    }\n",
    "                for atom in residue:\n",
    "                    atom_name = atom.get_name()\n",
    "                    element = atom.element\n",
    "                    coord = atom.get_coord()\n",
    "                    atom_type = None\n",
    "                    if res_name in amino_acid_atoms:\n",
    "                        atom_type_info = amino_acid_atoms[res_name].get(atom_name)\n",
    "                        if atom_type_info:\n",
    "                            atom_types = atom_type_info.split(',')\n",
    "                            atom_type = atom_types  # list\n",
    "                    residues[res_key]['atoms'].append({\n",
    "                        'atom_name': atom_name,\n",
    "                        'element': element,\n",
    "                        'coord': coord,\n",
    "                        'atom_type': atom_type\n",
    "                    })\n",
    "    return residues\n",
    "\n",
    "# Step 2: Compute Residue-Residue Interactions\n",
    "def compute_residue_interactions(residues, interaction_criteria):\n",
    "    interactions = []\n",
    "    residue_keys = list(residues.keys())\n",
    "    for i in range(len(residue_keys)):\n",
    "        for j in range(i+1, len(residue_keys)):\n",
    "            res1 = residues[residue_keys[i]]\n",
    "            res2 = residues[residue_keys[j]]\n",
    "            # Compute all atom pairs between res1 and res2\n",
    "            for atom1 in res1['atoms']:\n",
    "                for atom2 in res2['atoms']:\n",
    "                    distance = np.linalg.norm(atom1['coord'] - atom2['coord'])\n",
    "                    # Check each interaction type\n",
    "                    for interaction, criteria in interaction_criteria.items():\n",
    "                        type1 = criteria['atomic_type1']\n",
    "                        type2 = criteria['atomic_type2']\n",
    "                        min_dist = criteria['min_dist']\n",
    "                        max_dist = criteria['max_dist']\n",
    "                        \n",
    "                        # Check if atom types match\n",
    "                        if atom1['atom_type'] and atom2['atom_type']:\n",
    "                            # Since atom_type can be a list, check intersection\n",
    "                            if type1 in atom1['atom_type'] and type2 in atom2['atom_type']:\n",
    "                                if min_dist <= distance <= max_dist:\n",
    "                                    interactions.append({\n",
    "                                        'res1': residue_keys[i],\n",
    "                                        'res2': residue_keys[j],\n",
    "                                        'interaction_type': interaction,\n",
    "                                        'distance': distance\n",
    "                                    })\n",
    "    return interactions\n",
    "\n",
    "\n",
    "# Step 3: Generate Graph Network\n",
    "def generate_residue_graph(residues, residue_interactions):\n",
    "    G = nx.Graph()\n",
    "    # Add residues as nodes\n",
    "    for res_key, res_info in residues.items():\n",
    "        G.add_node(res_key, residue=res_info['residue'], res_id=res_info['res_id'], \n",
    "                   atom_types=[atom['atom_type'] for atom in res_info['atoms'] if atom['atom_type']])\n",
    "    \n",
    "    # Add edges based on interactions\n",
    "    for interaction in residue_interactions:\n",
    "        res1 = interaction['res1']\n",
    "        res2 = interaction['res2']\n",
    "        interaction_type = interaction['interaction_type']\n",
    "        distance = interaction['distance']\n",
    "        G.add_edge(res1, res2, interaction_type=interaction_type, distance=distance)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def map_surface_residues_to_graph(graph, surface_residues):\n",
    "    mapped_surface_nodes = []\n",
    "    for surface_res in surface_residues:\n",
    "        res_name = surface_res.get_resname()\n",
    "        res_id = surface_res.id[1]\n",
    "        surface_node_key = f\"{res_name}_{res_id}\"\n",
    "        \n",
    "        # Check if this residue is in the graph\n",
    "        if surface_node_key in graph.nodes:\n",
    "            mapped_surface_nodes.append(surface_node_key)\n",
    "    return mapped_surface_nodes\n",
    "\n",
    "\n",
    "def get_adjacent_nodes_and_edges(graph, surface_node):\n",
    "    # Get the adjacent residues (1-hop neighbors)\n",
    "    neighbors = list(graph.neighbors(surface_node))\n",
    "    edges = list(graph.edges(surface_node, data=True))  # Edges with data (interaction_type, distance, etc.)\n",
    "    \n",
    "    return neighbors, edges\n",
    "\n",
    "\n",
    "\n",
    "def get_adjacent_adjacent_nodes_and_edges(graph, neighbor_nodes):\n",
    "    adjacent_adjacent_nodes = set()  # To avoid duplicate entries\n",
    "    adjacent_adjacent_edges = []\n",
    "    \n",
    "    for neighbor in neighbor_nodes:\n",
    "        # Get neighbors of the neighbor (2-hop neighbors)\n",
    "        next_neighbors = list(graph.neighbors(neighbor))\n",
    "        next_edges = list(graph.edges(neighbor, data=True))\n",
    "        \n",
    "        adjacent_adjacent_nodes.update(next_neighbors)  # Add next level of neighbors\n",
    "        adjacent_adjacent_edges.extend(next_edges)      # Add next level of edges\n",
    "    \n",
    "    return list(adjacent_adjacent_nodes), adjacent_adjacent_edges\n",
    "\n",
    "\n",
    "atom_encoding = {\n",
    "        'ACP': 1,\n",
    "        'DON': 2,\n",
    "        'POS': 3,\n",
    "        'NEG': 4,\n",
    "        'HPB': 5,\n",
    "        'ARM': 6,        \n",
    "        None: 0  # Use 0 or any encoding for `None`\n",
    "    }\n",
    "  \n",
    "    \n",
    "def encode_atom_categories(atom_categories, atom_encoding, max_length):\n",
    "    # Convert dictionary of atom categories to a list of encoded values\n",
    "    encoded = [atom_encoding.get(value, 0) for value in atom_categories.values()]\n",
    "    # Pad encoded list to ensure it has `max_length`\n",
    "    return encoded + [0] * (max_length - len(encoded))\n",
    "\n",
    "\n",
    "# Helper function to extract node features and edges for each mapped surface node and adjacent nodes\n",
    "def extract_subgraph(graph, mapped_surface_nodes):\n",
    "    nodes, edges = [], []\n",
    "    max_atom_categories_length = max(len(amino_acid_atoms[res_name]) for res_name in amino_acid_atoms)\n",
    "\n",
    "    for surface_node in mapped_surface_nodes:\n",
    "        if surface_node in graph:\n",
    "            res_name, res_id = surface_node.split('_')\n",
    "            atom_categories = amino_acid_atoms[res_name]\n",
    "            encoded_atom_categories = encode_atom_categories(atom_categories, atom_encoding, max_atom_categories_length)\n",
    "            \n",
    "            # Convert each node feature list to a tensor with consistent length\n",
    "            node_tensor = torch.tensor([int(res_id)] + encoded_atom_categories, dtype=torch.float)\n",
    "            nodes.append(node_tensor)\n",
    "            \n",
    "        adjacent_nodes, adjacent_edges = get_adjacent_nodes_and_edges(graph, surface_node)\n",
    "        adjacent_adjacent_nodes, adjacent_adjacent_edges = get_adjacent_adjacent_nodes_and_edges(graph, adjacent_nodes)\n",
    "        \n",
    "        # Ensure edges are stored as integer pairs\n",
    "        edges.extend([\n",
    "            (int(edge[0].split('_')[1]), int(edge[1].split('_')[1]))\n",
    "            for edge in adjacent_edges\n",
    "        ])\n",
    "        edges.extend([\n",
    "            (int(edge[0].split('_')[1]), int(edge[1].split('_')[1]))\n",
    "            for edge in adjacent_adjacent_edges\n",
    "        ])\n",
    "        \n",
    "        for adj_node in adjacent_nodes + adjacent_adjacent_nodes:\n",
    "            if adj_node in graph:\n",
    "                res_name, res_id = adj_node.split('_')\n",
    "                atom_categories = amino_acid_atoms[res_name]\n",
    "                encoded_atom_categories = encode_atom_categories(atom_categories, atom_encoding, max_atom_categories_length)\n",
    "                \n",
    "                adj_node_tensor = torch.tensor([int(res_id)] + encoded_atom_categories, dtype=torch.float)\n",
    "                nodes.append(adj_node_tensor)\n",
    "                \n",
    "    nodes_tensor = torch.stack(nodes)\n",
    "    edges_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous()  # Ensure edges are in long format\n",
    "    \n",
    "    return nodes_tensor, edges_tensor\n",
    "\n",
    "\n",
    "# Define function to create a Data object for each protein graph from parsed PDB data\n",
    "def create_protein_data(graph_p1, graph_p2, surface_residues_p1, surface_residues_p2, class_label):\n",
    "    # Map surface residues to graph nodes and retrieve subgraphs\n",
    "    mapped_surface_nodes_p1 = map_surface_residues_to_graph(graph_p1, surface_residues_p1)\n",
    "    mapped_surface_nodes_p2 = map_surface_residues_to_graph(graph_p2, surface_residues_p2)\n",
    "    \n",
    "    # Get nodes and edges for subgraphs\n",
    "    nodes_p1, edges_p1 = extract_subgraph(graph_p1, mapped_surface_nodes_p1)\n",
    "    nodes_p2, edges_p2 = extract_subgraph(graph_p2, mapped_surface_nodes_p2)\n",
    "    \n",
    "    # Concatenate node features\n",
    "    x = torch.cat((nodes_p1, nodes_p2), dim=0)\n",
    "    \n",
    "    # Offset edges for p2 and concatenate\n",
    "    edge_index_p2 = edges_p2 + nodes_p1.size(0)  # Offset edges for p2 nodes\n",
    "    edge_index = torch.cat((edges_p1, edge_index_p2), dim=1)\n",
    "    \n",
    "    # Define label for the protein\n",
    "    y = torch.tensor([class_label], dtype=torch.long)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116ad790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GNN model\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        # Global pooling for graph-level embedding\n",
    "        x = global_mean_pool(x, batch)  # Pool over nodes for each graph\n",
    "        # Output layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Function to test the model\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c009a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    # Load the CSV dataset containing protein pairs and class labels\n",
    "    df_pairs = pd.read_csv('demo.csv')  # Load your dataset here (replace demo.csv with actual file)\n",
    "    logger.info(\"Dataset loaded successfully\")\n",
    "    \n",
    "    # Iterate over protein pairs and generate feature vectors\n",
    "    data_list = []\n",
    "    \n",
    "    logger.info(\"Start creating protein data object\")\n",
    "    for index, row in df_pairs.iterrows():\n",
    "        protein1 = row['P1']  # Assuming 'P1' is the protein 1 column\n",
    "        protein2 = row['P2']  # Assuming 'P2' is the protein 2 column\n",
    "        class_label = row['class']\n",
    "        \n",
    "        \n",
    "        # P1\n",
    "        surface_residues_p1 = calculate_sasa('files/' + protein1)\n",
    "        residues_p1 = parse_pdb('files/' + protein1)\n",
    "        interactions_p1 = compute_residue_interactions(residues_p1, interaction_criteria)\n",
    "        graph_p1 = generate_residue_graph(residues_p1, interactions_p1)\n",
    "        \n",
    "        \n",
    "        # P2\n",
    "        surface_residues_p2 = calculate_sasa('files/' + protein2)\n",
    "        residues_p2 = parse_pdb('files/' + protein2)\n",
    "        interactions_p2 = compute_residue_interactions(residues_p2, interaction_criteria)\n",
    "        graph_p2 = generate_residue_graph(residues_p2, interactions_p2)\n",
    "        \n",
    "        \n",
    "        # Create the Data object\n",
    "        protein_data = create_protein_data(graph_p1, graph_p2, surface_residues_p1, surface_residues_p2, class_label)\n",
    "        data_list.append(protein_data)\n",
    "        \n",
    "    # Save the data_list to a file\n",
    "    torch.save(data_list, 'data_list3.pt')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b7b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 18:06:58,154 - INFO - Dataset loaded successfully\n",
      "2024-11-09 18:06:58,155 - INFO - Start creating protein data object\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ede28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the two .pt files\n",
    "data_list_1 = torch.load('data_list1.pt')\n",
    "data_list_2 = torch.load('data_list2.pt')\n",
    "\n",
    "# Merge the lists\n",
    "merged_data_list = data_list_1 + data_list_2  # Concatenates the two lists\n",
    "\n",
    "# Save the merged list to a new .pt file\n",
    "torch.save(merged_data_list, 'merged_data_list.pt')\n",
    "\n",
    "print(\"Merged data saved to 'merged_data_list.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():    \n",
    "    logger.info(\"Start defining hyperparameters\")\n",
    "    # Define hyperparameters\n",
    "    input_dim = 10  # Replace with the actual feature dimension of nodes\n",
    "    hidden_dim = 64\n",
    "    output_dim = 3  # Number of classes in your multi-class classification\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    logger.info(\"Start Initializing the model\")\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    model = GNN(input_dim, hidden_dim, output_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    logger.info(\"Start loading into DataLoader\")\n",
    "    # Load the data_list from the saved file\n",
    "    loaded_data_list = torch.load('data_list.pt')\n",
    "    loader = DataLoader(loaded_data_list, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Example: Iterating through DataLoader and printing batch info\n",
    "    for batch in loader:\n",
    "        print(\"Batch node features shape:\", batch.x.shape)\n",
    "        print(\"Batch edge indices shape:\", batch.edge_index.shape)\n",
    "        print(\"Batch labels:\", batch.y)\n",
    "        print(\"------------\")\n",
    "        \n",
    "    logger.info(\"Start training loop\")\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, loader, optimizer, criterion)\n",
    "        accuracy = test(model, loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
